---
title: "Lab 7"
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, error = FALSE,
                      warning = FALSE, message = FALSE, 
                      fig.align = "center",
                      R.options = list(max.print=100))

# Edit the file starting below
```

### Enter the names of the group members here:

**This assignment is due by the end of the day. Only one student in the group submits a pdf file on Gradescope.**

*For all questions, include the R commands/functions that you used to find your answer (show R chunk). Answers without supporting code will not receive credit. Write full sentences to describe your findings.*

## Introduction

In this lab, you will explore one of the published novels of Jane Austen, accessible through the `janeaustenr` package. While doing this Lab, you may want to **review the notes for the lecture on Text Data Analysis** (the slides can be found in the Lab GitHub repository).

First install the packages you will need to complete this lab. Use the `install.packages()` function to install the following packages:

* janeaustenr
* tidytext
* wordcloud
* RColorBrewer
* ggwordcloud

Remember to only run `install.packages()` in the console. Do not use that function inside your Rmd file.

After you have installed the packages, you should be able to load them using the code in the following code chunk.


```{r, warning=FALSE}
# Remember to install new packages with install.packages("name") in the console
# if you haven't already done that.
library(janeaustenr)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
library(ggwordcloud)
```

Let's take a quick look at the books available:

```{r}
# Take a quick look
austen_books() |>
  group_by(book) |>
  summarize(nb_lines = n())
```

The goal of the lab is to conduct some sentiment analysis for one of these books.


### Question 1: (6 pts)

After calling all books with `austen_books()`, choose to keep **only one of the books** (it doesn't matter which one).

Then let's do some cleaning:

-   create a variable `chapter` with the following structure: `mutate(chapter = cumsum(str_detect(text, "")))`. Fill in the `""` with a regular expression to **find the lines mentioning the chapter sections**. *Hint: Take a look at how the chapter sections appear in your data first.*

-   get rid of the lines for chapter 0,

-   get rid of the empty lines,

-   get rid of the lines showing the chapter sections. *Hint: str_detect() with regex would be useful again!*. Note: some novels also have volumes. Get rid of those lines as well.

Save the resulting dataset as an object called `book`.

```{r}
# your code goes below (make sure to edit comment)

```

How many chapters were contained in the `book` you chose?

```{r}
# your code goes below (make sure to edit comment)

```

**Your answer goes here. Write sentences in bold.**


### Question 2: (3 pts)

Next, we will split each line into words (this is sometimes called "tokenization"). One very convenient function to do this is `unnest_tokens(word, text)` (we do not need to specify the token as the default token is `words`). What are the 10 most common words in the book you chose? Do they reveal any pattern?

```{r}
# your code goes below (make sure to edit comment)

```

**Your answer goes here. Write sentences in bold.**


### Question 3: (4 pts)

After getting the words by themselves, we will want to get rid of the stop words with the `SMART` lexicon:

```{r}
# Recall the SMART lexicon
SMARTstops <- stop_words |> 
    filter(lexicon == "SMART")
```

Split each line in `book` into words with `unnest_tokens()` (like you did in Question 2) and use one of the joining function to get rid of stop words. Call the resulting dataset `words_books`.

```{r}
# your code goes below (make sure to edit comment)

```

Find the 10 most common words in `words_book` and display those in a word cloud. Do you notice any pattern in those words?

```{r}
# your code goes below (make sure to edit comment)

```

**Your answer goes here. Write sentences in bold.**


### Question 4: (5 pts)

Let's take a look at the sentiments associated with words in the book and how these sentiments change as the story goes. We will consider positive/negative words from the `sentiments` object:

```{r}
# Sentiments lexicon
head(sentiments)
```

Follow those steps to keep track of the sentiments as the story goes:

1.  Use a joining function to only keep the words in `words_book` that are associated with either a positive/negative sentiment.

2.  Find the number of words with positive and negative sentiment per chapter. *Hint: use group_by() with two variables.*

3.  Use a pivot function to have the number of positive words and negative words in separate columns.

4.  Find the proportion of words with a positive sentiment.

5.  Create a `ggplot` with `geom_line()` and `geom_smooth()` to represent the proportion of words with a positive sentiment across the chapters.

How do the sentiments evolve as the story goes?

```{r}
# your code goes below (make sure to edit comment)

```

**Your answer goes here. Write sentences in bold.**


### Formatting: (2 pts)

Comment your code, write full sentences, knit your file, and select all appropriate pages in Gradescope for each question!
